{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PO6vGmAtMKOh"
   },
   "source": [
    "# Lab 2: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXZCyKzgMKOi"
   },
   "source": [
    "*OpenAI gym FrozenLake environment*\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LC-IV7YmMKOj"
   },
   "source": [
    "In this assignment, you will be implementing a value iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1647844967797,
     "user": {
      "displayName": "­황태현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08611587733822633308"
     },
     "user_tz": -540
    },
    "id": "WMRPWkoXMKOj",
    "outputId": "a20994d0-31e7-4264-8397-e9f8543811c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version 1.24.2\n",
      "gym version 0.26.2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "print(\"numpy version\", np.__version__)\n",
    "print(\"gym version\", gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1647844969492,
     "user": {
      "displayName": "­황태현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08611587733822633308"
     },
     "user_tz": -540
    },
    "id": "99HHdrBXMKOk"
   },
   "outputs": [],
   "source": [
    "env=gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1647844970878,
     "user": {
      "displayName": "­황태현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08611587733822633308"
     },
     "user_tz": -540
    },
    "id": "D4uizPB8MKOk"
   },
   "outputs": [],
   "source": [
    "## DO NOT CHANGE THIS CELL\n",
    "\n",
    "def argmax(env, V, pi, action, s, gamma):\n",
    "    # do not have attribute `env.env.nA`\n",
    "    # e = np.zeros(env.env.nA)\n",
    "    # for a in range(env.env.nA):\n",
    "    nA = env.env.action_space.n\n",
    "    e = np.zeros(nA)\n",
    "    for a in range(nA):             # iterate for every action possible \n",
    "        q=0\n",
    "        P = np.array(env.env.P[s][a])                   \n",
    "        (x,y) = np.shape(P)                             # for Bellman Equation \n",
    "        \n",
    "        for i in range(x):                              # iterate for every possible states\n",
    "            s_= int(P[i][1])                            # S' - Sprime - possible succesor states\n",
    "            p = P[i][0]                                 # Transition Probability P(s'|s,a) \n",
    "            r = P[i][2]                                 # Reward\n",
    "            \n",
    "            q += p*(r+gamma*V[s_])                      # calculate action_ value q(s|a)\n",
    "            e[a] = q\n",
    "            \n",
    "    m = np.argmax(e) \n",
    "    action[s]=m                                         # Take index which has maximum value \n",
    "    pi[s][m] = 1                                        # update pi(a|s) \n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwJD1lBDMKOk"
   },
   "source": [
    "Your task is to complete the \"bellman_opt_update\" function below, which servces as a subroutine for the value iteration method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`is_slippery`: True/False. If True will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions.\n",
    "\n",
    "`Action Space`\n",
    "- 0: LEFT\n",
    "- 1: DOWN\n",
    "- 2: RIGHT\n",
    "- 3: UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q1IObFjBMKOl"
   },
   "outputs": [],
   "source": [
    "def bellman_opt_update(env, V, s, gamma):  # update the stae_value V[s] by taking \n",
    "    \"\"\"\n",
    "    max_a \\sum_{s', r} p(s', r|s, a)[r + \\gamma V(s')]\n",
    "    \"\"\"\n",
    "    nA = env.env.action_space.n\n",
    "    q_pi = np.zeros(nA)\n",
    "    for a in range(nA):\n",
    "        q_pi[a] = np.sum([p * (r + gamma * V[s_]) for p, s_, r, _ in env.env.P[s][a]])\n",
    "    V[s] = np.max(q_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1647844974897,
     "user": {
      "displayName": "­황태현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08611587733822633308"
     },
     "user_tz": -540
    },
    "id": "bn6uxXw0MKOl"
   },
   "outputs": [],
   "source": [
    "## DO NOT CHANGE THIS CELL\n",
    "\n",
    "def value_iteration(env, gamma, theta):\n",
    "    nS = env.env.observation_space.n\n",
    "    nA = env.env.action_space.n\n",
    "    V = np.zeros(nS)                              # initialize v(0) to arbitory value, my case \"zeros\"\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(nS):                       # iterate for all states\n",
    "            v = V[s]\n",
    "            bellman_opt_update(env, V, s, gamma)   # update state_value with bellman optimality update\n",
    "            delta = max(delta, abs(v - V[s]))             # assign the change in value per iteration to delta  \n",
    "        if delta < theta:                                       \n",
    "            break                                         # if change gets to negligible \n",
    "                                                          # --> converged to optimal value         \n",
    "    pi = np.zeros((nS, nA)) \n",
    "    action = np.zeros((nS), dtype=np.int32)\n",
    "    for s in range(nS):\n",
    "        pi = argmax(env, V, pi, action, s, gamma)         # extract optimal policy using action value \n",
    "        \n",
    "    return V, pi, action   # optimal value funtion, optimal policy with one-hot encoding (pi), optimal policy with discrete number (action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1647844977035,
     "user": {
      "displayName": "­황태현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08611587733822633308"
     },
     "user_tz": -540
    },
    "id": "_teSxydCMKOl",
    "outputId": "c42846ad-ca37-4711-e968-54cd474a7262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " agent succeeded to reach goal 825 out of 1000 Episodes using this policy \n",
      " success rate: 0.825\n"
     ]
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS CELL\n",
    "actions_label = {0: '<', 1: 'v', 2: '>', 3: '^'}\n",
    "gamma = 0.99\n",
    "theta = 0.000001\n",
    "seed = 1\n",
    "\n",
    "env.reset(seed=seed) # env.seed(1)\n",
    "V, pi, action = value_iteration(env, gamma, theta) # note \"action\" is optimal policy with discrete action number\n",
    "\n",
    "#initialize episodic structure\n",
    "num_episodes=1000\n",
    "episode_max_length=10000\n",
    "\n",
    "\n",
    "e=0\n",
    "for i_episode in range(num_episodes):\n",
    "    s, prob = env.reset()\n",
    "    for t in range(episode_max_length):\n",
    "        s_, reward, done, *info = env.step(action=action[s])\n",
    "        # just for printing\n",
    "        # print(f'[E-{i_episode} | {t}] {s}, {actions_label[action[s]]}, {s_} | r={reward}, done? {done}')\n",
    "        s = s_\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                e +=1\n",
    "            break\n",
    "print(\" agent succeeded to reach goal {} out of {} Episodes using this policy \".format(e+1, num_episodes))\n",
    "print(\" success rate:\", (e+1)/num_episodes)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xKSiGBZMKOm"
   },
   "source": [
    "### Save this notebook file (after you modify and run the code) and submit \"lab2.ipynb\" on eTL\n",
    "\n",
    "Your success rate should be above 70%"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rl_2023spring-FuEMe-5O",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "207baee91ecd117027fd93803a84ae863535bc689f1835e13b0a544143db48c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

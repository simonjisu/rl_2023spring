{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_vcd9X9gTiZY"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1651212719232,
     "user": {
      "displayName": "¬≠Ìô©ÌÉúÌòÑ",
      "userId": "08611587733822633308"
     },
     "user_tz": -540
    },
    "id": "ptS4IWGJTsEa",
    "outputId": "a87d06b5-5321-4095-e48b-7519a769b114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YvnMR2VVTiZj"
   },
   "source": [
    "## DQN (Deep Q Network)\n",
    "\n",
    "In this lab, we will apply deep learning as function approximations in reinforcement learning. \n",
    "\n",
    "Reference: DQN https://arxiv.org/abs/1312.5602\n",
    "\n",
    "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon-$greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$ \n",
    "\n",
    "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
    "\n",
    "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*there are alternative parameterizations here*). Let $Q_\\theta(s,a)$ be the output of the network, to estimate the optimal action value function in state $s$ and take action $a$ (and follow optimal policy thereafter). \n",
    "\n",
    "$$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "P41VY29LTiZk"
   },
   "source": [
    "### Bellman optimality equation\n",
    "\n",
    "We will use Bellman optimality equation to find $\\theta$ such that the above approximation holds better. Recall that for optimal Q function $Q^\\ast(s,a)$ the following holds for all $(s,a)$\n",
    "\n",
    "$$Q^\\ast(s_t,a_t) = \\max_a \\mathbb{E}\\big[r_t + \\gamma Q^\\ast(s_{t+1},a)\\big]$$\n",
    "\n",
    "Hence a natural objective to consider is \n",
    "\n",
    "$$\\min_\\theta\\  (Q_\\theta(s_t,a_t) - \\max_a \\mathbb{E}\\big[r_t + \\gamma Q_\\theta(s_{t+1},a)\\big])^2$$\n",
    "\n",
    "Let us first build a neural network $Q_\\theta(s,a)$ as required above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aKonQSRKTiZl"
   },
   "outputs": [],
   "source": [
    "# define neural net Q_\\theta(s,a) as a class\n",
    "\n",
    "class Qfunction(keras.Model):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        hidden_dims: list containing output dimension of hidden layers \n",
    "        \"\"\"\n",
    "        super(Qfunction, self).__init__()\n",
    "\n",
    "        # Layer weight initializer\n",
    "        initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.)\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer = keras.layers.InputLayer(input_shape=(obssize,))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.hidden_layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            # TODO: define each hidden layers\n",
    "            layer = keras.layers.Dense(hidden_dim, activation='relu',\n",
    "                                      kernel_initializer=initializer)\n",
    "            self.hidden_layers.append(layer) \n",
    "        \n",
    "        # Output Layer : \n",
    "        # TODO: Define the output layer.\n",
    "        self.output_layer = keras.layers.Dense(actsize) \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, states):\n",
    "        ########################################################################\n",
    "        # TODO: You SHOULD implement the model's forward pass\n",
    "        o = self.input_layer(states)\n",
    "        for layer in self.hidden_layers:\n",
    "            o = layer(o)\n",
    "        qvalues = self.output_layer(o)\n",
    "        return qvalues\n",
    "        ########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SfiuGhwo3Jdg"
   },
   "outputs": [],
   "source": [
    "# Wrapper class for training Qfunction and updating weights (target network) \n",
    "\n",
    "class DQN(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        optimizer: \n",
    "        \"\"\"\n",
    "        self.qfunction = Qfunction(obssize, actsize, hidden_dims)\n",
    "        self.optimizer = optimizer\n",
    "        self.obssize = obssize\n",
    "        self.actsize = actsize\n",
    "\n",
    "    def _predict_q(self, states, actions):\n",
    "        \"\"\"\n",
    "        states represent s_t\n",
    "        actions represent a_t\n",
    "        \"\"\"\n",
    "        ########################################################################\n",
    "        # TODO: Define the logic for calculate  Q_\\theta(s,a)\n",
    "        q_pred = self.qfunction(states)\n",
    "        q_pred = tf.gather(q_pred, actions, batch_dims=1)\n",
    "        return q_pred\n",
    "        ########################################################################\n",
    "        \n",
    "\n",
    "    def _loss(self, Qpreds, targets):\n",
    "        \"\"\"\n",
    "        Qpreds represent Q_\\theta(s,a)\n",
    "        targets represent the terms E[r+gamma Q] in Bellman equations\n",
    "\n",
    "        This function is OBJECTIVE function\n",
    "        \"\"\"\n",
    "        return tf.math.reduce_mean(tf.square(Qpreds - targets))\n",
    "\n",
    "\n",
    "    def compute_Qvalues(self, states):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to the neural net, states should have\n",
    "        size [numsamples, obssize], where numsamples is the number of samples\n",
    "        output: Q values for these states. The output should have size \n",
    "        [numsamples, actsize] as numpy array\n",
    "        \"\"\"\n",
    "        inputs = np.atleast_2d(states.astype('float32'))\n",
    "        return self.qfunction(inputs)\n",
    "\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            Qpreds = self._predict_q(states, actions)\n",
    "            loss = self._loss(Qpreds, targets)\n",
    "        variables = self.qfunction.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss\n",
    "\n",
    "    def update_weights(self, from_network):\n",
    "        \"\"\"\n",
    "        We need a subroutine to update target network \n",
    "        i.e. to copy from principal network to target network. \n",
    "        This function is for copying  ùúÉ‚ÜêùúÉtarget \n",
    "        \"\"\"\n",
    "        \n",
    "        from_var = from_network.qfunction.trainable_variables\n",
    "        to_var = self.qfunction.trainable_variables\n",
    "        \n",
    "        for v1, v2 in zip(from_var, to_var):\n",
    "            v2.assign(v1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QYpgktSQTiZl"
   },
   "source": [
    "Now that we have $Q_\\theta(s,a)$ we can execute policies in the environment as follows ($\\epsilon-$greedy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YVmqzhs0TiZm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\.virtualenvs\\rl_2023spring-9SpTBrNN\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\simon\\.virtualenvs\\rl_2023spring-9SpTBrNN\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward under current policy 10.0000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\.virtualenvs\\rl_2023spring-9SpTBrNN\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# # just pseudocode\n",
    "# # Please comment out this cell when implementing the code\n",
    "# raise ValueError(\"cannot attemp to run pseudocode\")\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "epsilon = .1\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "rewardsum = 0\n",
    "dqn = DQN(4, 2, [10, 5], optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "while not done:\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        Q = dqn.compute_Qvalues(obs)\n",
    "        action = np.argmax(Q)  # need some tweeking of code here\n",
    "    \n",
    "    obs, reward, done, info, _ = env.step(action)\n",
    "    rewardsum += reward\n",
    "    \n",
    "print(\"reward under current policy {:.4f} \".format(rewardsum)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Fx_1UmTiZm"
   },
   "source": [
    "We can hence collect a bunch of samples $(s_t,a_t,r_t,s_{t+1})$, and compute the targets using the current network. Let the target be $d_i$ as the $i$th target\n",
    "\n",
    "$$d_i = \\max_a r_t + \\gamma Q_\\theta(s_{t+1},a)$$\n",
    "\n",
    "And then feed this value into the computational graph and minimize the Bellman error loss. This procedure has been shown to be fairly unstable. The reference paper has offered two techniques to stabilize the training process: target network and replay buffer.\n",
    "\n",
    "**Replay Buffer**\n",
    "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$, when we minimize the Bellman error. When optimizing the Bellman error loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples from buffer $(s_t,a_t,r_t,s_{t+1}) \\sim R$ and then compute\n",
    "loss \n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - \\max_a (r_i + \\gamma Q_\\theta(s_i^\\prime,a))^2$$\n",
    "\n",
    "and update parameters using backprop.\n",
    "\n",
    "**Target Network**\n",
    "Maintain a target network in addition to the original pricipal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta_{\\text{target}}$ is copied from the principal network every $\\tau$ time steps. Target network is used to compute the targets for update\n",
    "\n",
    "$$d_i = \\max_a r_t + \\gamma Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
    "\n",
    "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UwrViYZ2TiZn"
   },
   "source": [
    "Hence several critical parts of the complete pseudocode for DQN is as follows:\n",
    "\n",
    "**Initialization**: principal network $Q_\\theta(s,a)$, target network $Q_{\\theta^{-}}(s,a)$. Replay buffer $R = \\{\\}$ (empty). \n",
    "\n",
    "**At each time step $t$**: execute action using $\\epsilon-$greedy based on the principal network $Q_\\theta(s,a)$. To update $\\theta$: sample $N$ tuples $(s_i,a_i,r_i,s_i^\\prime) \\sim R$, compute empirical loss \n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - \\max_a (r_i + \\gamma Q_{\\theta^{-}}(s_i^\\prime,a))^2$$\n",
    "\n",
    "and update parameter $\\theta$ using backprop (just take one gradient step).\n",
    "\n",
    "**Update target network**: Every $\\tau$ time steps, update target network by copying $\\theta_{\\text{target}} \\leftarrow \\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HiAwMOvRTiZn"
   },
   "outputs": [],
   "source": [
    "# Implement replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        if(self.number > self.maxlength):\n",
    "            self.pop()\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        inds = np.random.choice(len(self.buffer), batchsize, replace=False)\n",
    "        return [self.buffer[idx] for idx in inds]\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WIVMPznZTiZo"
   },
   "source": [
    "Now that we have all the ingredients for DQN, we can write the main procedure to train DQN on a given environment. The implementation is straightforward if you follow the pseudocode. Refer to the pseudocode pdf for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5626382,
     "status": "ok",
     "timestamp": 1651219040495,
     "user": {
      "displayName": "¬≠Ìô©ÌÉúÌòÑ",
      "userId": "08611587733822633308"
     },
     "user_tz": -540
    },
    "id": "c1eHVbfBTiZp",
    "outputId": "84613f7b-150d-404c-c14a-f7b1fe7a39d7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3295d1cfe1bc4a5193ea4551b66221b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\.virtualenvs\\rl_2023spring-9SpTBrNN\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\simon\\.virtualenvs\\rl_2023spring-9SpTBrNN\\lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 ave reward 13.0\n",
      "iteration 10 ave reward 14.2\n",
      "iteration 20 ave reward 52.1\n",
      "iteration 30 ave reward 49.2\n",
      "iteration 40 ave reward 74.2\n",
      "iteration 50 ave reward 79.1\n",
      "iteration 60 ave reward 61.4\n",
      "iteration 70 ave reward 105.3\n",
      "iteration 80 ave reward 147.9\n",
      "iteration 90 ave reward 151.0\n",
      "iteration 100 ave reward 165.7\n",
      "iteration 110 ave reward 184.6\n",
      "iteration 120 ave reward 171.6\n",
      "iteration 130 ave reward 150.3\n",
      "iteration 140 ave reward 171.9\n",
      "iteration 150 ave reward 199.8\n",
      "iteration 160 ave reward 187.4\n",
      "iteration 170 ave reward 205.0\n",
      "iteration 180 ave reward 166.7\n",
      "iteration 190 ave reward 160.2\n",
      "iteration 200 ave reward 141.5\n",
      "iteration 210 ave reward 179.1\n",
      "iteration 220 ave reward 110.4\n",
      "iteration 230 ave reward 40.8\n",
      "iteration 240 ave reward 190.4\n",
      "iteration 250 ave reward 169.1\n",
      "iteration 260 ave reward 169.6\n",
      "iteration 270 ave reward 186.4\n",
      "iteration 280 ave reward 220.1\n",
      "iteration 290 ave reward 203.0\n",
      "iteration 300 ave reward 135.2\n",
      "iteration 310 ave reward 152.5\n",
      "iteration 320 ave reward 171.6\n",
      "iteration 330 ave reward 190.2\n",
      "iteration 340 ave reward 183.0\n",
      "iteration 350 ave reward 198.6\n",
      "iteration 360 ave reward 222.6\n",
      "iteration 370 ave reward 249.9\n",
      "iteration 380 ave reward 158.0\n",
      "iteration 390 ave reward 274.1\n",
      "Solved after 392 episodes.\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO: Set the required parameters. All parameters can be tuned by yourself.\n",
    "from tqdm.notebook import tqdm  # need to install ipywidgets\n",
    "lr = 0.0015 # 5e-3  # learning rate for gradient update \n",
    "batchsize = 64  # batchsize for buffer sampling\n",
    "maxlength = 10000  # max number of tuples held by buffer\n",
    "envname = \"CartPole-v0\"  # environment name\n",
    "tau = 400  # time steps for target update\n",
    "episodes = 5000  # number of episodes to run\n",
    "initialize = 100  # initial time steps before start updating\n",
    "# epsilon = .05  # constant for exploration\n",
    "epsilon = 0.5\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.95 # discount\n",
    "hidden_dims=[32, 32] # hidden dimensions\n",
    "################################################################################\n",
    "\n",
    "# initialize environment\n",
    "env = gym.make(envname)\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n\n",
    "\n",
    "# optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# initialize networks\n",
    "Qprincipal = DQN(obssize, actsize, hidden_dims, optimizer)\n",
    "Qtarget = DQN(obssize, actsize, hidden_dims, optimizer)\n",
    "\n",
    "# initialization of buffer\n",
    "buffer = ReplayBuffer(maxlength)\n",
    "\n",
    "################################################################################\n",
    "# TODO: Complete the main iteration\n",
    "# CartPole-v0 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials.\n",
    "\n",
    "rrecord = []\n",
    "loss = None\n",
    "totalstep = 0\n",
    "global_logger = tqdm(leave=False)\n",
    "for ite in range(episodes):\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    epsilon = max(epsilon * (epsilon_decay**ite), epsilon_min)\n",
    "    global_logger.set_description(f\"[{ite:05}] epsilon: {epsilon:.6f}\")\n",
    "    while not done:\n",
    "        \n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_values_principal = Qprincipal.compute_Qvalues(obs)\n",
    "            action = np.argmax(q_values_principal)\n",
    "\n",
    "        next_obs, reward, done, *_ = env.step(action)\n",
    "        buffer.append((obs, action, reward, next_obs, done))  # append s,a,r,s',done\n",
    "\n",
    "        rsum += reward\n",
    "        \n",
    "        # Update\n",
    "        if totalstep >= initialize:\n",
    "            batch = buffer.sample(batchsize)\n",
    "            s, a, r, s_prime, ds = map(np.array, list(zip(*batch)))\n",
    "            \n",
    "            q_values_target = Qtarget.compute_Qvalues(s_prime)\n",
    "            target = r + gamma * np.amax(q_values_target, axis=1) * (1 - ds)\n",
    "\n",
    "            loss = Qprincipal.train(s, a, target)\n",
    "\n",
    "        if totalstep % tau == 0:\n",
    "            Qtarget.update_weights(Qprincipal)\n",
    "\n",
    "        obs = next_obs\n",
    "        totalstep += 1\n",
    "\n",
    "        log_str = f'[{totalstep}] R={np.mean(rrecord[-10:]):.4f}'\n",
    "        if loss is not None:\n",
    "            log_str += f', L={loss:.6f}'\n",
    "            \n",
    "        global_logger.set_postfix_str(log_str)\n",
    "\n",
    "    global_logger.update(1)\n",
    "################################################################################\n",
    "    \n",
    "    ## DO NOT CHANGE THIS PART!\n",
    "    rrecord.append(rsum)\n",
    "    if ite % 10 == 0:\n",
    "        print('iteration {} ave reward {}'.format(ite, np.mean(rrecord[-10:])))\n",
    "\n",
    "    ave100 = np.mean(rrecord[-100:])   \n",
    "    if  ave100 > 195.0:\n",
    "        print(\"Solved after %d episodes.\"%ite)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 1065,
     "status": "ok",
     "timestamp": 1651219115528,
     "user": {
      "displayName": "¬≠Ìô©ÌÉúÌòÑ",
      "userId": "08611587733822633308"
     },
     "user_tz": -540
    },
    "id": "D6UUdjTyxjsr",
    "outputId": "4956f9dc-9427-4f88-a708-5139075e645f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rrecord' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# plot [episode, reward] history\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m x \u001b[39m=\u001b[39m [i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(rrecord))]\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mplot(x, rrecord)\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mepisode rewards\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rrecord' is not defined"
     ]
    }
   ],
   "source": [
    "# plot [episode, reward] history\n",
    "x = [i+1 for i in range(len(rrecord))]\n",
    "plt.plot(x, rrecord)\n",
    "plt.title('episode rewards')\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('rewards')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nRf96xABTiZp"
   },
   "source": [
    "Once you run the code, save the notebook file of name \"Lab 3_your name.ipynb\" and submit on eTL.\n",
    "\n",
    "# Do NOT submit as zip files."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

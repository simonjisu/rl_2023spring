{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "\n",
    "from src.argument_parser import get_parser, parse_args_str\n",
    "from src.GridWorldMDP.utils import draw_path, generate_demonstrations, init_grid_world\n",
    "\n",
    "PARSER = get_parser()\n",
    "\n",
    "ARGS = \"\"\"\n",
    "--exp_name test\n",
    "--height 6\n",
    "--width 6\n",
    "--gamma 0.8\n",
    "--act_random 0.3\n",
    "--n_trajs 10\n",
    "--l_traj 6\n",
    "--learning_rate 0.1\n",
    "--n_iters 100\n",
    "--alpha 0.1\n",
    "--n_query 1\n",
    "--r_max 1\n",
    "--error 0.01\n",
    "--grad_clip 0.5\n",
    "--weight_decay 10\n",
    "--hiddens 64 32\n",
    "--device cpu\n",
    "--verbose 2\n",
    "\"\"\"\n",
    "\n",
    "args = parse_args_str(PARSER, ARGS)\n",
    "args.exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Grid World\n",
      "[INFO] Getting ground truth values and policy via value teration\n",
      "s=(4, 1), a=r, r=0.0, s'=(4, 2) -> \n",
      "s=(4, 2), a=r, r=0.0, s'=(4, 3) -> \n",
      "s=(4, 3), a=r, r=0.0, s'=(4, 4) -> \n",
      "s=(4, 4), a=s, r=1.0, s'=(4, 4) -> \n",
      "s=(4, 4), a=s, r=1.0, s'=(4, 4) -> \n",
      "s=(4, 4), a=s, r=1.0, s'=(4, 3)\n"
     ]
    }
   ],
   "source": [
    "init_start_pos = [(4, 1)]\n",
    "coor_rates = [\n",
    "    ((args.height-2, args.width-2), 1.0), \n",
    "    ((0, args.width-1), 0.5), \n",
    "    ((1, 1), 0.5)\n",
    "]\n",
    "gw, P_a, rewards_gt, values_gt, policy_gt = init_grid_world(args, coor_rates)\n",
    "\n",
    "# use identity matrix as feature\n",
    "feat_map = np.eye(args.height * args.width)\n",
    "if init_start_pos is None:\n",
    "    trajs = generate_demonstrations(gw, policy_gt, \n",
    "                                    n_trajs=args.n_query, \n",
    "                                    len_traj=args.l_traj, \n",
    "                                    rand_start=True, \n",
    "                                    start_pos=None)\n",
    "else:\n",
    "    if isinstance(init_start_pos[0], list) or isinstance(init_start_pos[0], tuple):\n",
    "        # multiple start points\n",
    "        trajs = []\n",
    "        for sp in init_start_pos:\n",
    "            t = generate_demonstrations(gw, policy_gt, \n",
    "                                        n_trajs=args.n_query, \n",
    "                                        len_traj=args.l_traj, \n",
    "                                        rand_start=False, \n",
    "                                        start_pos=sp)\n",
    "            trajs.extend(t)\n",
    "    else:\n",
    "        # type(init_start_pos[0]) == int\n",
    "        trajs = generate_demonstrations(gw, policy_gt, \n",
    "                                        n_trajs=args.n_query, \n",
    "                                        len_traj=args.l_traj, \n",
    "                                        rand_start=False, \n",
    "                                        start_pos=init_start_pos)\n",
    "print(draw_path(trajs[0], gw))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a size=torch.Size([1, 2]), b size=torch.Size([2, 1])\n",
      "a @ b =\n",
      " tensor([[11.]], grad_fn=<MmBackward0>)\n",
      "c size=torch.Size([1, 1])\n",
      "None\n",
      "a.grad =\n",
      " tensor([[-3., -4.]])\n",
      "b.grad =\n",
      " tensor([[-1.],\n",
      "        [-2.]])\n",
      "\n",
      "a_grad =\n",
      " tensor([[-3., -4.]])\n",
      "b_grad =\n",
      " tensor([[-1.],\n",
      "        [-2.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "\n",
    "a = torch.tensor([[1., 2.]]).float().requires_grad_()\n",
    "b = torch.tensor([[3.], [4.]]).float().requires_grad_()\n",
    "print(f'a size={a.size()}, b size={b.size()}')\n",
    "c = torch.matmul(a, b)\n",
    "print(f'a @ b =\\n {c}')\n",
    "print(f'c size={c.size()}')\n",
    "\n",
    "print(c.backward(-torch.ones_like(c), retain_graph=True))\n",
    "print(f'a.grad =\\n {a.grad}')\n",
    "print(f'b.grad =\\n {b.grad}')\n",
    "print()\n",
    "a_grad = autograd.grad(c, a, grad_outputs=-torch.ones_like(c), retain_graph=True)[0]\n",
    "print(f'a_grad =\\n {a_grad}')\n",
    "b_grad = autograd.grad(c, b, grad_outputs=-torch.ones_like(c), retain_graph=True)[0]\n",
    "print(f'b_grad =\\n {b_grad}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check function is working\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\theta) = \\log P(\\mathcal{D}, \\theta \\vert r) = \\log P(\\mathcal{D} \\vert r) + \\log P(\\theta \\vert r) $$\n",
    "\n",
    "* first term: loglikelihood of trajs given rewards\n",
    "* second term: parameter l2 loss\n",
    "\n",
    "$$ \\dfrac{\\partial \\mathcal{L}}{\\partial \\theta} = \\dfrac{\\partial \\mathcal{L}_{\\mathcal{D}}}{\\partial \\theta} + \\dfrac{\\partial \\mathcal{L}_{\\theta}}{\\partial \\theta}$$\n",
    "\n",
    "first term can be decomposed as:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\dfrac{\\partial \\mathcal{L}_{\\mathcal{D}}}{\\partial \\theta} &= \\dfrac{\\partial \\mathcal{L}_{\\mathcal{D}}}{\\partial r} \\dfrac{\\partial r}{\\partial \\theta} \\\\\n",
    "&= (\\mu_{\\mathcal{D}} - \\Bbb{E}[\\mu]) \\cdot \\dfrac{\\partial g(f, \\theta)}{\\partial \\theta}\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $r = g(f, \\theta)$, $f$ is the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.,\n",
       "       1., 0.])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.deepmaxent_irl import DeepIRLFC, demo_svf, compute_state_visition_freq\n",
    "from src.GridWorldMDP.value_iteration import value_iteration\n",
    "\n",
    "device = torch.device('cpu')\n",
    "n_states = args.height * args.width\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = DeepIRLFC(input_dim=feat_map.shape[1], hiddens=[3, 3]).to(device)\n",
    "\n",
    "mu_D = demo_svf(trajs, n_states)\n",
    "inputs = torch.from_numpy(feat_map).float().to(device)\n",
    "mu_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0275  0.01    0.028   0.0278 -0.0097 -0.0675 -0.0177 -0.0824 -0.0662\n",
      " -0.0164  0.0196 -0.0329 -0.0114 -0.0172 -0.0055 -0.0141 -0.0122 -0.0514\n",
      "  0.0153 -0.0151 -0.0065 -0.051  -0.0644 -0.0724  0.0047 -0.0111 -0.0811\n",
      " -0.0152  0.0291 -0.0023 -0.0282 -0.0455 -0.0222  0.0067 -0.0502 -0.0366]\n"
     ]
    }
   ],
   "source": [
    "rewards = model(inputs)\n",
    "rewards_numpy = rewards.view(-1).detach().numpy()\n",
    "print(rewards_numpy.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poilcy = [2. 2. 2. 1. 3. 3. 0. 1. 1. 1. 4. 3. 0. 3. 2. 1. 1. 3. 3. 3. 3. 0. 0. 0.\n",
      " 1. 3. 2. 2. 4. 3. 1. 1. 2. 1. 1. 1.]\n",
      "mu_D = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# during iteration\n",
    "# approximate value iteration\n",
    "_, policy = value_iteration(P_a, rewards_numpy, gamma=args.gamma, alpha=args.alpha, error=args.error, deterministic=True)\n",
    "# propagate policy\n",
    "mu_exp = compute_state_visition_freq(P_a, trajs, policy, deterministic=True)\n",
    "print(f'poilcy = {policy}')\n",
    "print(f'mu_D = {mu_D.round(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.8180e-02,\n",
       "        -4.9768e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.3830e-02,\n",
       "        -2.3854e+00, -2.1033e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        -4.0740e-03,  8.0147e-01, -2.4139e-02,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00, -2.5955e-04,  9.8884e-01, -1.9051e-03,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00, -2.0356e-04,  1.9873e+00, -1.5267e-03,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00, -9.8496e-06,  9.9955e-01,\n",
       "        -4.9248e-05])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_r = mu_D - mu_exp\n",
    "grad_r = torch.from_numpy(grad_r).float().view(-1, 1).to(device)\n",
    "grad_r.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00, -1.0467e-06,  1.1638e-01, -5.5125e-06],\n",
       "        [ 0.0000e+00,  0.0000e+00,  1.2956e-06, -1.3760e-01,  7.2481e-06],\n",
       "        [ 0.0000e+00,  0.0000e+00,  1.6622e-06, -1.5529e-01,  7.6223e-06]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_grads = torch.autograd.grad(rewards, model.parameters(), grad_outputs=-grad_r, retain_graph=True)\n",
    "all_grads[0][-5:, -5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3350, grad_fn=<SumBackward0>)\n",
      "tensor([[ 0.1440, -0.1080, -0.0767, -0.1164, -0.1561],\n",
      "        [ 0.0386,  0.1034,  0.1600, -0.1284, -0.0611],\n",
      "        [-0.0419, -0.1588, -0.0030, -0.1255, -0.1286]])\n",
      "grad l2\n",
      "tensor([[ 1.4400, -1.0803, -0.7672, -1.0480, -1.5609],\n",
      "        [ 0.3862,  1.0340,  1.6003, -1.4220, -0.6108],\n",
      "        [-0.4195, -1.5877, -0.0300, -1.4104, -1.2856]])\n"
     ]
    }
   ],
   "source": [
    "l2_loss = torch.stack([torch.sum(p.pow(2))/2 for p in model.parameters()]).sum()\n",
    "l2_grad = torch.autograd.grad(l2_loss, model.parameters(), retain_graph=True)\n",
    "print(l2_loss)\n",
    "print(l2_grad[0][-5:, -5:])\n",
    "all_grad_l2 = [args.weight_decay*l2_grad[i]+all_grads[i] for i in range(len(all_grads))]\n",
    "print('grad l2')\n",
    "print(all_grad_l2[0][-5:, -5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04346468, -0.03260752, -0.02315768, -0.03163309, -0.04711509],\n",
       "       [ 0.01165621,  0.03120885,  0.04830385, -0.0429206 , -0.01843552],\n",
       "       [-0.01266133, -0.04792169, -0.00090416, -0.04257084, -0.03880388]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "grad_theta, _ = tf.clip_by_global_norm(all_grad_l2, args.grad_clip)\n",
    "grad_theta[0].numpy()[-5:, -5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04346468, -0.03260752, -0.02315768, -0.03163309, -0.04711509],\n",
       "       [ 0.01165621,  0.03120885,  0.04830385, -0.0429206 , -0.01843552],\n",
       "       [-0.01266133, -0.04792169, -0.00090416, -0.04257084, -0.03880388]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/clip_by_global_norm\n",
    "clip_norm = args.grad_clip\n",
    "global_norm = torch.sqrt(torch.stack([torch.norm(g).pow(2) for g in all_grad_l2]).sum())\n",
    "clip_coef = clip_norm / max(global_norm, clip_norm)\n",
    "grad_theta = [g * clip_coef for g in all_grad_l2]\n",
    "grad_theta[0].numpy()[-5:, -5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param\n",
      "[[ 0.144  -0.108  -0.0767 -0.1164 -0.1561]\n",
      " [ 0.0386  0.1034  0.16   -0.1284 -0.0611]\n",
      " [-0.0419 -0.1588 -0.003  -0.1255 -0.1286]]\n",
      "\n",
      "grad\n",
      "[[ 0.      0.     -0.      0.1164 -0.    ]\n",
      " [ 0.      0.      0.     -0.1376  0.    ]\n",
      " [ 0.      0.      0.     -0.1553  0.    ]]\n",
      "apply gradient without l2\n",
      "[[ 0.144  -0.108  -0.0767 -0.1281 -0.1561]\n",
      " [ 0.0386  0.1034  0.16   -0.1147 -0.0611]\n",
      " [-0.0419 -0.1588 -0.003  -0.11   -0.1286]]\n",
      "grad with l2\n",
      "[[ 1.44   -1.0803 -0.7672 -1.048  -1.5609]\n",
      " [ 0.3862  1.034   1.6003 -1.422  -0.6108]\n",
      " [-0.4195 -1.5877 -0.03   -1.4104 -1.2856]]\n",
      "apply gradient with l2\n",
      "[[ 0.      0.      0.     -0.0116  0.    ]\n",
      " [ 0.     -0.     -0.      0.0138 -0.    ]\n",
      " [ 0.      0.     -0.      0.0155 -0.    ]]\n",
      "grad with clip and l2\n",
      "[[ 0.0435 -0.0326 -0.0232 -0.0316 -0.0471]\n",
      " [ 0.0117  0.0312  0.0483 -0.0429 -0.0184]\n",
      " [-0.0127 -0.0479 -0.0009 -0.0426 -0.0388]]\n",
      "apply gradient with clip and l2\n",
      "[[ 0.1397 -0.1048 -0.0744 -0.1133 -0.1514]\n",
      " [ 0.0375  0.1003  0.1552 -0.1241 -0.0592]\n",
      " [-0.0407 -0.154  -0.0029 -0.1213 -0.1247]]\n"
     ]
    }
   ],
   "source": [
    "param = list(model.parameters())[0]\n",
    "print('param')\n",
    "print(f'{param.detach().cpu().numpy().round(4)[-5:, -5:]}')\n",
    "print()\n",
    "print('grad')\n",
    "print(f'{all_grads[0].detach().cpu().numpy().round(4)[-5:, -5:]}')\n",
    "print('apply gradient without l2')\n",
    "print(f'{(param - args.learning_rate * all_grads[0]).detach().cpu().numpy().round(4)[-5:, -5:]}')\n",
    "print('grad with l2')\n",
    "print(f'{all_grad_l2[0].detach().cpu().numpy().round(4)[-5:, -5:]}')\n",
    "print('apply gradient with l2')\n",
    "print(f'{(param - args.learning_rate * all_grad_l2[0]).detach().cpu().numpy().round(4)[-5:, -5:]}')\n",
    "print('grad with clip and l2')\n",
    "print(f'{grad_theta[0].detach().cpu().numpy().round(4)[-5:, -5:]}')\n",
    "print('apply gradient with clip and l2')\n",
    "print(f'{(param - args.learning_rate * grad_theta[0]).detach().cpu().numpy().round(4)[-5:, -5:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param\n",
      "tensor([[ 0.1440, -0.1080, -0.0767, -0.1164, -0.1561],\n",
      "        [ 0.0386,  0.1034,  0.1600, -0.1284, -0.0611],\n",
      "        [-0.0419, -0.1588, -0.0030, -0.1255, -0.1286]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "grad\n",
      " tensor([[ 0.0000e+00,  0.0000e+00, -1.0467e-06,  1.1638e-01, -5.5125e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  1.2956e-06, -1.3760e-01,  7.2481e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  1.6622e-06, -1.5529e-01,  7.6223e-06]])\n",
      "apply gradient\n",
      " tensor([[ 0.1440, -0.1080, -0.0767, -0.1281, -0.1561],\n",
      "        [ 0.0386,  0.1034,  0.1600, -0.1147, -0.0611],\n",
      "        [-0.0419, -0.1588, -0.0030, -0.1100, -0.1286]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = DeepIRLFC(input_dim=feat_map.shape[1], hiddens=[3, 3]).to(device)\n",
    "param = list(model.parameters())[0]\n",
    "print('param')\n",
    "print(f'{param[-5:, -5:]}')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, weight_decay=0.0)\n",
    "optimizer.zero_grad()\n",
    "rewards = model(inputs)\n",
    "rewards_numpy = rewards.view(-1).detach().numpy()\n",
    "_, policy = value_iteration(P_a, rewards_numpy, gamma=args.gamma, alpha=args.alpha, error=args.error, deterministic=True)\n",
    "# propagate policy\n",
    "mu_exp = compute_state_visition_freq(P_a, trajs, policy, deterministic=True)\n",
    "grad_r = mu_D - mu_exp\n",
    "grad_r = torch.from_numpy(grad_r).float().view(-1, 1).to(device)\n",
    "\n",
    "rewards.backward(-grad_r, retain_graph=True)\n",
    "# nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "# using optimizer\n",
    "optimizer.step()\n",
    "param = list(model.parameters())[0]\n",
    "print('grad\\n', param.grad[-5:, -5:])\n",
    "print('apply gradient\\n', param[-5:, -5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param\n",
      "tensor([[ 0.1440, -0.1080, -0.0767, -0.1164, -0.1561],\n",
      "        [ 0.0386,  0.1034,  0.1600, -0.1284, -0.0611],\n",
      "        [-0.0419, -0.1588, -0.0030, -0.1255, -0.1286]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "manual grad\n",
      "\n",
      "tensor([[ 0.0000e+00,  7.4506e-09,  1.1176e-07, -1.1638e-02,  5.5134e-07],\n",
      "        [ 0.0000e+00, -7.4506e-09, -1.3411e-07,  1.3760e-02, -7.2271e-07],\n",
      "        [ 0.0000e+00,  1.4901e-08, -1.6601e-07,  1.5529e-02, -7.5996e-07]],\n",
      "       grad_fn=<CloneBackward0>)\n",
      "grad\n",
      " tensor([[ 0.0000e+00,  0.0000e+00, -1.0467e-06,  1.1638e-01, -5.5125e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  1.2956e-06, -1.3760e-01,  7.2481e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  1.6622e-06, -1.5529e-01,  7.6223e-06]])\n",
      "apply gradient\n",
      " tensor([[ 0.0000e+00,  7.4506e-09,  1.1176e-07, -1.1638e-02,  5.5134e-07],\n",
      "        [ 0.0000e+00, -7.4506e-09, -1.3411e-07,  1.3760e-02, -7.2271e-07],\n",
      "        [ 0.0000e+00,  1.4901e-08, -1.6601e-07,  1.5529e-02, -7.5996e-07]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[True, True, True, True, True],\n",
      "        [True, True, True, True, True],\n",
      "        [True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = DeepIRLFC(input_dim=feat_map.shape[1], hiddens=[3, 3]).to(device)\n",
    "param = list(model.parameters())[0]\n",
    "print('param')\n",
    "print(f'{param[-5:, -5:]}')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "optimizer.zero_grad()\n",
    "rewards = model(inputs)\n",
    "rewards_numpy = rewards.view(-1).detach().numpy()\n",
    "_, policy = value_iteration(P_a, rewards_numpy, gamma=args.gamma, alpha=args.alpha, error=args.error, deterministic=True)\n",
    "# propagate policy\n",
    "mu_exp = compute_state_visition_freq(P_a, trajs, policy, deterministic=True)\n",
    "grad_r = mu_D - mu_exp\n",
    "grad_r = torch.from_numpy(grad_r).float().view(-1, 1).to(device)\n",
    "\n",
    "rewards.backward(-grad_r, retain_graph=True)\n",
    "\n",
    "l2_loss = torch.stack([torch.sum(p.pow(2))/2 for p in model.parameters()]).sum()\n",
    "l2_grad = torch.autograd.grad(l2_loss, model.parameters(), retain_graph=True)\n",
    "# manual \n",
    "print('manual grad\\n')\n",
    "mg = (param - args.learning_rate*(param.grad + args.weight_decay*l2_grad[0]))[-5:, -5:].clone()\n",
    "print(mg)\n",
    "# using optimizer\n",
    "optimizer.step()\n",
    "param = list(model.parameters())[0]\n",
    "print('grad\\n', param.grad[-5:, -5:])\n",
    "print('apply gradient\\n', param[-5:, -5:])\n",
    "print(param[-5:, -5:] == mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_theta(args, rewards, model, grad_r):\n",
    "    all_grads = torch.autograd.grad(rewards, model.parameters(), grad_outputs=-grad_r, retain_graph=True)\n",
    "    l2_loss = torch.stack([torch.sum(p.pow(2))/2 for p in model.parameters()]).sum()\n",
    "    l2_grad = torch.autograd.grad(l2_loss, model.parameters(), retain_graph=True)\n",
    "    all_grad_l2 = [args.weight_decay*l2_grad[i]+all_grads[i] for i in range(len(all_grads))]\n",
    "    global_norm = torch.sqrt(torch.stack([torch.norm(g).pow(2) for g in all_grad_l2]).sum())\n",
    "    clip_coef = args.grad_clip / max(global_norm, args.grad_clip)\n",
    "    grad_theta = [g * clip_coef for g in all_grad_l2]\n",
    "    return grad_theta, l2_loss\n",
    "\n",
    "def apply_gradient(model, grad_theta, args):\n",
    "    for p, g in zip(model.parameters(), grad_theta):\n",
    "        p.data -= args.learning_rate * g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param\n",
      "tensor([[ 0.1440, -0.1080, -0.0767, -0.1164, -0.1561],\n",
      "        [ 0.0386,  0.1034,  0.1600, -0.1284, -0.0611],\n",
      "        [-0.0419, -0.1588, -0.0030, -0.1255, -0.1286]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "grad\n",
      " tensor([[ 0.0435, -0.0326, -0.0232, -0.0316, -0.0471],\n",
      "        [ 0.0117,  0.0312,  0.0483, -0.0429, -0.0184],\n",
      "        [-0.0127, -0.0479, -0.0009, -0.0426, -0.0388]])\n",
      "apply gradient with clip and l2\n",
      " tensor([[ 0.1397, -0.1048, -0.0744, -0.1133, -0.1514],\n",
      "        [ 0.0375,  0.1003,  0.1552, -0.1241, -0.0592],\n",
      "        [-0.0407, -0.1540, -0.0029, -0.1213, -0.1247]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# without optimizer\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = DeepIRLFC(input_dim=feat_map.shape[1], hiddens=[3, 3]).to(device)\n",
    "param = list(model.parameters())[0]\n",
    "print('param')\n",
    "print(f'{param[-5:, -5:]}')\n",
    "\n",
    "rewards = model(inputs)\n",
    "rewards_numpy = rewards.view(-1).detach().numpy()\n",
    "_, policy = value_iteration(P_a, rewards_numpy, gamma=args.gamma, alpha=args.alpha, error=args.error, deterministic=True)\n",
    "# propagate policy\n",
    "mu_exp = compute_state_visition_freq(P_a, trajs, policy, deterministic=True)\n",
    "grad_r = mu_D - mu_exp\n",
    "grad_r = torch.from_numpy(grad_r).float().view(-1, 1).to(device)\n",
    "grad_theta, l2_loss = get_grad_theta(args, rewards, model, grad_r)\n",
    "apply_gradient(model, grad_theta, args)\n",
    "print('grad\\n', grad_theta[0][-5:, -5:])\n",
    "print('apply gradient with clip and l2\\n', param[-5:, -5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param\n",
      "tensor([[ 0.1440, -0.1080, -0.0767, -0.1164, -0.1561],\n",
      "        [ 0.0386,  0.1034,  0.1600, -0.1284, -0.0611],\n",
      "        [-0.0419, -0.1588, -0.0030, -0.1255, -0.1286]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "grad\n",
      " tensor([[ 0.0000e+00,  0.0000e+00, -1.9331e-07,  2.1494e-02, -1.0181e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  2.3927e-07, -2.5413e-02,  1.3386e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  3.0697e-07, -2.8679e-02,  1.4077e-06]])\n",
      "apply gradient with clip and l2\n",
      " tensor([[ 0.0000e+00,  7.4506e-09,  2.2352e-08, -2.1494e-03,  1.0431e-07],\n",
      "        [ 0.0000e+00, -7.4506e-09, -2.9802e-08,  2.5413e-03, -1.2666e-07],\n",
      "        [ 0.0000e+00,  1.4901e-08, -3.0734e-08,  2.8679e-03, -1.3411e-07]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = DeepIRLFC(input_dim=feat_map.shape[1], hiddens=[3, 3]).to(device)\n",
    "param = list(model.parameters())[0]\n",
    "print('param')\n",
    "print(f'{param[-5:, -5:]}')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "optimizer.zero_grad()\n",
    "rewards = model(inputs)\n",
    "rewards_numpy = rewards.view(-1).detach().numpy()\n",
    "_, policy = value_iteration(P_a, rewards_numpy, gamma=args.gamma, alpha=args.alpha, error=args.error, deterministic=True)\n",
    "# propagate policy\n",
    "mu_exp = compute_state_visition_freq(P_a, trajs, policy, deterministic=True)\n",
    "grad_r = mu_D - mu_exp\n",
    "grad_r = torch.from_numpy(grad_r).float().view(-1, 1).to(device)\n",
    "\n",
    "rewards.backward(-grad_r, retain_graph=True)\n",
    "nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "# using optimizer\n",
    "optimizer.step()\n",
    "print('grad\\n', param.grad[-5:, -5:])\n",
    "print('apply gradient with clip and l2\\n', param[-5:, -5:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initialize Grid World\n",
      "[INFO] Getting ground truth values and policy via value teration\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1245584786dd45c597d0384fe074992a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.argument_parser import get_parser, parse_args_str\n",
    "from src.GridWorldMDP.utils import draw_path, generate_demonstrations, init_grid_world\n",
    "from src.GridWorldMDP.value_iteration import value_iteration\n",
    "from src.GridWorldMDP.policy_iteration import finite_policy_iteration, policy_evaluation\n",
    "from src.deepmaxent_irl import DeepIRLFC, demo_svf, compute_state_visition_freq, deepmaxent_irl\n",
    "\n",
    "PARSER = get_parser()\n",
    "\n",
    "ARGS = \"\"\"\n",
    "--exp_name test\n",
    "--height 6\n",
    "--width 6\n",
    "--gamma 0.9\n",
    "--act_random 0.3\n",
    "--n_trajs 200\n",
    "--l_traj 6\n",
    "--learning_rate 0.02\n",
    "--n_iters 20\n",
    "--alpha 0.1\n",
    "--n_query 1\n",
    "--r_max 1\n",
    "--error 0.001\n",
    "--grad_clip 100\n",
    "--weight_decay 10\n",
    "--hiddens 3 3\n",
    "--device cpu\n",
    "--verbose 2\n",
    "\"\"\"\n",
    "# learning 조절 잘해야함\n",
    "args = parse_args_str(PARSER, ARGS)\n",
    "args.exp_name\n",
    "\n",
    "coor_rates = [\n",
    "    ((args.height-2, args.width-2), 1.0), \n",
    "    ((0, args.width-1), 0.5), \n",
    "    ((1, 1), 0.5)\n",
    "]\n",
    "gw, P_a, rewards_gt, values_gt, policy_gt = init_grid_world(args, coor_rates)\n",
    "# use identity matrix as feature\n",
    "feat_map = np.eye(args.height * args.width)\n",
    "trajs = generate_demonstrations(gw, policy_gt, \n",
    "                                n_trajs=args.n_trajs, \n",
    "                                len_traj=args.l_traj, \n",
    "                                rand_start=True, \n",
    "                                start_pos=None)\n",
    "\n",
    "rewards, policy, l2_loss = deepmaxent_irl(feat_map, P_a, trajs, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 1. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. ])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35287154, 0.47049537, 0.88217884, 0.5293073 , 0.9998027 ,\n",
       "       0.5293073 , 0.6469312 , 0.05881192, 0.5881192 , 0.47049537,\n",
       "       0.17643577, 0.17643577, 0.17643577, 0.05881192, 0.6469312 ,\n",
       "       0.5881192 , 0.2940596 , 0.17643577, 0.88217884, 0.82336694,\n",
       "       0.47049537, 0.17643577, 0.764555  , 0.5293073 , 0.11762384,\n",
       "       0.82336694, 0.82336694, 0.17643577, 0.764555  , 0.        ,\n",
       "       0.41168347, 0.82336694, 0.5881192 , 0.05881192, 0.2940596 ,\n",
       "       0.17643577], dtype=float32)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_2023spring-9SpTBrNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
